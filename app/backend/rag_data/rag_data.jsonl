{"chunk_id": "DIARIZATIONLM.pdf-0", "text": "DIARIZATION LM: S PEAKER DIARIZATION POST-\nPROCESSING WITH LARGE LANGUAGE MODELS\nQuan Wang\u22c6 Yiling Huang\u22c6 Guanlong Zhao\u22c6\nEvan Clark Wei Xia Hank Liao\nGoogle LLC, USA \u22c6Equal contribution\nB quanw@google.com\nABSTRACT\nIn this paper, we introduce DiarizationLM, a framework to leverage large language\nmodels (LLM) to post-process the outputs from a speaker diarization system.\nVarious goals can be achieved with the proposed framework, such as improving\nthe readability of the diarized transcript, or reducing the word diarization error\nrate (WDER). In this framework, the outputs of the automatic speech recognition\n(ASR) and speaker diarization systems are represented as a compact textual format,\nwhich is included in the prompt to an optionally finetuned LLM. The outputs of the\nLLM can be used as the refined diarization results with the desired enhancement.\nAs a post-processing step, this framework can be easily applied to any off-the-shelf\nASR and speaker diarization systems without retraining e", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-1", "text": "refined diarization results with the desired enhancement.\nAs a post-processing step, this framework can be easily applied to any off-the-shelf\nASR and speaker diarization systems without retraining existing components. Our\nexperiments show that a finetuned PaLM 2-S model can reduce the WDER by\nrel. 55.5% on the Fisher telephone conversation dataset, and rel. 44.9% on the\nCallhome English dataset.\nCode: https://github.com/google/speaker-id/tree/master/DiarizationLM\nModel: https://huggingface.co/google/DiarizationLM-8b-Fisher-v2\nDemo: https://huggingface.co/spaces/diarizers-community/DiarizationLM-GGUF\nCONTENTS\n1 Introduction 2\n2 Motivating example 4\n3 DiarizationLM 4\n3.1 System overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n3.2 Prompt builder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n3.3 Completion parser . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n3.4 Transcript-Preserving Speaker Transfer .", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-2", "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n3.3 Completion parser . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n3.4 Transcript-Preserving Speaker Transfer . . . . . . . . . . . . . . . . . . . . . . . 5\n3.5 LLM finetuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n3.5.1 Flavor 1: hyp2ora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n3.5.2 Flavor 2: deg2ref . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n3.5.3 Flavor 3: mixed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n4 Experiments 8\n4.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n4.2 Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1\narXiv:2401.03506v11  [eess.AS]  8 Jan 2025\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\n4.3 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-3", "text": ". . . 8\n1\narXiv:2401.03506v11  [eess.AS]  8 Jan 2025\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\n4.3 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n4.4 Zero-shot and one-shot baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n4.5 Evaluation results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n4.6 Case studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n5 Discussion and future work 11\n6 Related work 14\n6.1 Speaker diarization post-processing . . . . . . . . . . . . . . . . . . . . . . . . . 14\n6.2 Speaker diarization with semantic information . . . . . . . . . . . . . . . . . . . . 15\n6.3 Speaker diarization with LLM . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n7 Conclusion 15\nA Open source models 20\nA.1 google/DiarizationLM-13b-Fisher-v1 . . . . . . . . . . . . . . . . . 20\nA.2 google/DiarizationLM-8b-Fisher-v1 . . . . .", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-4", "text": ". . . . . . . . . . . . . . . . . 15\n7 Conclusion 15\nA Open source models 20\nA.1 google/DiarizationLM-13b-Fisher-v1 . . . . . . . . . . . . . . . . . 20\nA.2 google/DiarizationLM-8b-Fisher-v1 . . . . . . . . . . . . . . . . . . 20\nA.3 google/DiarizationLM-8b-Fisher-v2 . . . . . . . . . . . . . . . . . . 21\nB Other diarization capabilities of LLMs 21\nB.1 Autofilling speaker names . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nB.2 Autofilling speaker roles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nB.3 Replacing the orchestration module . . . . . . . . . . . . . . . . . . . . . . . . . 22\n1 I NTRODUCTION\nSpeaker diarization is the task of partitioning speech into homogeneous segments according to speaker\nidentities, answering the question \u201cwho spoken when\u201d [1, 2]. Typical speaker diarization systems can\nbe roughly categorized into two groups: modularized systems and end-to-end systems. A modularized\nspeaker diarization system usually consists of multi", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-5", "text": "\u201d [1, 2]. Typical speaker diarization systems can\nbe roughly categorized into two groups: modularized systems and end-to-end systems. A modularized\nspeaker diarization system usually consists of multiple separately trained components including voice\nactivity detection (V AD) [3, 4, 5, 6], speaker turn detection [7, 8], speaker encoder [9, 10, 11], and a\nclustering algorithm, which can be either unsupervised [12, 13, 14, 15, 16, 17] or supervised [18, 19].\nEnd-to-end systems , on the other hand, directly optimize the entire system on diarization errors by\nintroducing a permutation invariant loss function [20, 21, 22, 23].\nIn many real world applications such as meeting summarization, call center analysis, mobile recorder\napps [24], and video captioning, knowing \u201cwho spoke when\u201d is not sufficient. Speaker labels are more\ninterpretable and meaningful when they are associated with speech transcripts. Various solutions\nhave been proposed to directly address the problem of \u201cwho spoke what\u201d,", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-6", "text": "cient. Speaker labels are more\ninterpretable and meaningful when they are associated with speech transcripts. Various solutions\nhave been proposed to directly address the problem of \u201cwho spoke what\u201d, including jointly training\nspeech recognition and speaker diarization [ 25], speaker-attributed automatic speech recognition\n(SA-ASR) [26, 27, 28, 29], target speaker automatic speech recognition (TS-ASR) [30, 31, 32, 33]\nand word-level end-to-end neural speaker diarization [34].\nIn practice, however, most production speech systems still consist of separately trained ASR models\nand speaker diarization models, with various considerations including:\n1. Modularized development and deployment:ASR and speaker diarization systems are usually\ntrained on different datasets, and potentially using different modeling framework, by different\nresearch teams.\n2\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\ngood morning how are you\nspk1 spk2\nASR outputs\nSpeaker diarization", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-7", "text": "rent modeling framework, by different\nresearch teams.\n2\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\ngood morning how are you\nspk1 spk2\nASR outputs\nSpeaker diarization \noutputs\n(a)\ngood morning how are\nspk1 spk2\nASR outputs\nSpeaker diarization \noutputs spk1\nyou\n(b)\nFigure 1: The orchestration module associates each word from the ASR transcript with a speaker label\nfrom the speaker diarization outputs. (a) In this example, all words are associated with the correct\nspeaker labels (green arrows). The words \u201cgood\u201d, \u201cmorning\u201d, and \u201care\u201d and \u201cyou\u201d are associated\nwith the only speaker label that overlap with them. The word \u201chow\u201d overlaps with both spk1 and\nspk2, but has bigger overlaps with spk2, thus is associated with spk2. The word \u201cyou\u201d does not\noverlap with any speaker, but is closest to spk2, thus is associated with spk2. (b) In this example, two\nwords are associated with wrong speaker labels (red arrows) due to inconsistent timing information\nfrom the t", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-8", "text": "ny speaker, but is closest to spk2, thus is associated with spk2. (b) In this example, two\nwords are associated with wrong speaker labels (red arrows) due to inconsistent timing information\nfrom the two systems. The word \u201chow\u201d is mistakenly associated with spk1, since spk1 has more\noverlap with this word than spk2. The word \u201cyou\u201d is mistakenly associated with spk1, since spk1 is\ncloser to this word than spk2.\n2. Potential quality regression on ASR:ASR has many more use cases than speaker diarization.\nJoint modeling of ASR and speaker diarization usually has worse Word Error Rates (WER)\nthan ASR-only models, thus is not acceptable in many applications.\n3. Flexibility: Combining separately trained ASR models and speaker diarization models is a\nvery flexible solution. As long as the ASR model provides word timing information, it can\nbe combined with almost any speaker diarization model, either unsupervised or supervised,\neither modularized or end-to-end trained.\nWe refer to the combinatio", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-9", "text": "provides word timing information, it can\nbe combined with almost any speaker diarization model, either unsupervised or supervised,\neither modularized or end-to-end trained.\nWe refer to the combination of ASR transcripts and speaker diarization results as anorchestration\nmodule (in some other work [35], this process is called \u201creconciliation\u201d). In this module, each word\nfrom the ASR transcript is associated with a speaker label. A typical orchestration algorithm works\nas follows: (1) If the word segment overlaps with at least one speaker segment, then this word is\nassociated with the speaker that has the biggest temporal overlap with this word; (2) otherwise if this\nword segment does not overlap with any speaker segment, then it is associated with the speaker that\nhas the smallest temporal distance to this word based on the segment boundaries. This orchestration\nalgorithm is illustrated in Fig. 1a.\nHowever, since ASR and speaker diarization are separately trained with usually different", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-10", "text": "distance to this word based on the segment boundaries. This orchestration\nalgorithm is illustrated in Fig. 1a.\nHowever, since ASR and speaker diarization are separately trained with usually different training\ndatasets and modeling approaches, the timing information from these two systems can be inconsistent,\nresulting in word diarization errors, as demonstrated with the example in Fig. 1b. Specifically, modern\nASR models are usually trained end-to-end without using the ground truth timing information, and\nthe word timing is inferred from the probability lattice of the decoder, which could be inaccurate.\nIn many cases, such errors can usually be fixed by leveraging semantic information from the ASR\ntranscripts. Take Fig. 1 as an example, simply by looking at the textual transcript \u201cgood morning how\nare you\u201d, if we know it consists of two speakers, we can easily tell which word comes from which\nspeaker confidently without using any acoustic speaker diarization system. In practice, diari", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-11", "text": "rning how\nare you\u201d, if we know it consists of two speakers, we can easily tell which word comes from which\nspeaker confidently without using any acoustic speaker diarization system. In practice, diarization\nerrors can be much more complicated than the simple example in Fig. 1. To handle such cases, we\npropose DiarizationLM, a framework to post-process the orchestrated ASR and speaker diarization\noutputs with a large language model (LLM). While the experiments performed in this paper mainly\nfocus on reducing word diarization errors using LLM, we also show examples how LLM can be used\nfor other purposes such as autofilling speaker names, autofilling speaker roles, or even completely\nreplacing the orchestration module in Appendix B.\n3\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\n2 M OTIVATING EXAMPLE\nHere is a small example where we construct a prompt (in blue) which consists of a brief instruction,\nand the diarization transcript with several errors. The p", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-12", "text": "rge Language Models\n2 M OTIVATING EXAMPLE\nHere is a small example where we construct a prompt (in blue) which consists of a brief instruction,\nand the diarization transcript with several errors. The prompt is sent to Google Bard [36] via its Web\nUI1. We can see that the completion generated by LLM (in orange) correctly moved the error words\nto the right speakers. This example can be easily reproduced with many off-the-shelf commercial or\nopen source instruction-finetuned LLMs, such as Google Bard [36] or OpenAI ChatGPT [37].\nPrompt:\nIn the speaker diarization transcript below, some words are potentially misplaced. Please correct\nthose words and move them to the right speaker. Directly show the corrected transcript without\nexplaining what changes were made or why you made those changes.\n<spk:1> Good morning Patrick, how\n<spk:2> are you? Good, good. How are you Tom? Pretty\n<spk:1> good. Going to work?\n<spk:2> Yes. Busy day. How are your kids? Do they go\n<spk:1> to school? Oh they are too", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-13", "text": "k:1> Good morning Patrick, how\n<spk:2> are you? Good, good. How are you Tom? Pretty\n<spk:1> good. Going to work?\n<spk:2> Yes. Busy day. How are your kids? Do they go\n<spk:1> to school? Oh they are too young for that. I sent them to daycare earlier\n<spk:2> today. Oh yeah I forgot about that.\nCompletion:\n<spk:1> Good morning Patrick, how are you?\n<spk:2> Good, good. How are you Tom?\n<spk:1> Pretty good. Going to work?\n<spk:2> Yes. Busy day. How are your kids? Do they go to school?\n<spk:1> Oh, they are too young for that. I sent them to daycare earlier today.\n<spk:2> Oh yeah, I forgot about that.\nApart from correcting diarization errors, we also found several other diarization capabilities of\nLLMs, such as autofilling speaker names, autofilling speaker roles, or even completely replacing the\norchestration module. Examples of these capabilities are shown in Appendix B. However, due to lack\nof relevant training and testing datasets, these capabilities are not thoroughly studied or evaluated", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-14", "text": "orchestration module. Examples of these capabilities are shown in Appendix B. However, due to lack\nof relevant training and testing datasets, these capabilities are not thoroughly studied or evaluated in\nthis paper. Here we focus on LLM\u2019s capability of reducing diarization errors.\n3 D IARIZATION LM\n3.1 S YSTEM OVERVIEW\nWe illustrate the DiarizationLM framework in Fig. 2. In this framework, the ASR and speaker\ndiarization systems are frozen, and their outputs are processed by the orchestration module to\nassociate a speaker label with each recognized word. The orchestrated diarization outputs are\nprocessed by a prompt buildermodule, which creates a compact textual representation of the diarized\ntranscript, segment it into shorter versions to fit the LLM input size limit, and apply prompt prefix\nand suffix. The prompts are then sent to a finetuned LLM, and the completions generated by the LLM\nwill be handled by a completion parsermodule, which truncates undesired outputs from the LLM,\ncom", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-15", "text": "fix\nand suffix. The prompts are then sent to a finetuned LLM, and the completions generated by the LLM\nwill be handled by a completion parsermodule, which truncates undesired outputs from the LLM,\ncombines the completions of multiple segments, and apply a transform (see Section 3.4) to preserve\nthe original transcripts of the ASR model.\n3.2 P ROMPT BUILDER\nThe output of the orchestration module is two sequences of equal length: a sequence of words, and a\nsequence of speaker labels. To fit it into a prompt, we use a compact textual representation, where\nspeaker tokens are only inserted in the beginning of the transcript, or when the speaker has changed.\nBelow is an example:\n1We used an internal version of Bard that is based on a larger model and supports more tokens than the public\nversion.\n4\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nOrchestration \nModule\nDiarizationLM\nPrompt \nBuilder\nCompletion \nParser\nASR\nSpeaker \nDiarization\nLLM\nWords with \ntiming", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-16", "text": "4\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nOrchestration \nModule\nDiarizationLM\nPrompt \nBuilder\nCompletion \nParser\nASR\nSpeaker \nDiarization\nLLM\nWords with \ntiming info\nSpeaker labels \nwith timing info\nWords with \nspeaker labels\n\ud83d\udd25\n\u2744\n\u2744\nFigure 2: Diagram of the proposed DiarizationLM framework.\nExample text representation of the prompt\nWord sequence: [\"good\", \"morning\", \"how\", \"are\", \"you\"]\nSpeaker sequence: [1, 1, 2, 2, 2]\nText representation: \"<spk:1> good morning <spk:2> how are you\"\nSince most LLMs have an input length limit, the text representation of an entire utterance may not\nfit this limit. In such cases, we recursively binary partition the word and speaker sequences in the\nmiddle, until all segments fit the the input length limit.\nWe also apply prefix and suffix to each prompt. The prefix is usually an instruction describing the\ntask for the LLM to perform, and the suffix is a sequence of tokens to indicate the end of the prompt.\n3.3 C OMPLET", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-17", "text": "refix and suffix to each prompt. The prefix is usually an instruction describing the\ntask for the LLM to perform, and the suffix is a sequence of tokens to indicate the end of the prompt.\n3.3 C OMPLETION PARSER\nEach prompt from the prompt builder will be sent to the finetuned LLM, which will generate a text\ncompletion for this prompt. First of all, we need to truncate any undesired outputs from the LLM.\nFor example, during the LLM finetuning, each completion may have a suffix to indicate the end of\nthe completion. Thus the suffix and any text generated after the suffix should be truncated from the\noriginal completion.\nAfter the truncation, we need to convert the text representation of the completion back to the word\nsequence and the speaker sequence format. If the text representation does not start with a speaker\ntoken, we either use the last speaker from the previous segment, or just use speaker 1 if it is the first\nsegment.\nNext, we concatenate the word sequences and speaker sequence", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-18", "text": "not start with a speaker\ntoken, we either use the last speaker from the previous segment, or just use speaker 1 if it is the first\nsegment.\nNext, we concatenate the word sequences and speaker sequences from all segments. However, the\nresulting concatenated word sequence may not be identical to the original word sequence from the\nASR model due to modifications by LLM. This is undesired and may hurt word error rate. Thus\nhere we need an algorithm to transfer the speaker labels from the concatenated speaker sequence to\nthe original word sequence from the ASR model. We will introduce this algorithm in the following\nsection.\n3.4 T RANSCRIPT -PRESERVING SPEAKER TRANSFER\nHere we describe an algorithm called Transcript-Preserving Speaker Transfer(TPST), which will\nbe used in several places in our proposed framework, including training data preparation and the\ncompletion parser module.\nAssume we have two sets of diarized transcript, referred to as \u201csource\u201d and \u201ctarget\u201d, each represented\nby two", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-19", "text": "proposed framework, including training data preparation and the\ncompletion parser module.\nAssume we have two sets of diarized transcript, referred to as \u201csource\u201d and \u201ctarget\u201d, each represented\nby two sequences of the same length: a sequence of words, and a sequence of speaker labels. The\npurpose of TPST is to transfer the speaker labels from the source sequences to the target sequences,\nsuch that:\n1. The transferred speaker label sequence has a 1-to-1 association with the target word se-\nquence.\n2. The transferred speaker labels are more consistent with the source speaker labels.\n5\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nAs an example, the concatenated word sequence from the completion parser module may not be\nidentical to the original word sequence from the ASR model. Thus we can treat the completion\nsequences as the source, and the original sequences from the orchestration module as the target, and\ntransfer the speaker labels. Finally, the Diariz", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-20", "text": "om the ASR model. Thus we can treat the completion\nsequences as the source, and the original sequences from the orchestration module as the target, and\ntransfer the speaker labels. Finally, the DiarizationLM outputs will be the original word sequence,\nassociated with the transferred speaker label sequence.\nThe detailed TPST algorithm is described in Algorithm 1. An implementation is open sourced on\nGitHub 2.\nAlgorithm 1 The transcript-preserving speaker transfer (TPST) algorithm.\ninputs\nSource word sequence of length N: wsrc = (wsrc\n1 , \u00b7\u00b7\u00b7 , wsrc\nN )\nSource speaker sequence of length N: ssrc = (ssrc\n1 , \u00b7\u00b7\u00b7 , ssrc\nN )\nTarget word sequence of length M: wtgt = (wtgt\n1 , \u00b7\u00b7\u00b7 , wtgt\nM )\nTarget speaker sequence of length M: stgt = (stgt\n1 , \u00b7\u00b7\u00b7 , stgt\nM )\noutputs\nTransferred speaker sequence of length M: stra = (stra\n1 , \u00b7\u00b7\u00b7 , stra\nM )\n1: procedure TPST( wsrc, ssrc, wtgt, stgt)\n2: Align wsrc to wtgt with the Levenshtein algorithm [38], resulting in a transform falign(\u00b7)\n3: sali \u2190 falign(ss", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-21", "text": "f length M: stra = (stra\n1 , \u00b7\u00b7\u00b7 , stra\nM )\n1: procedure TPST( wsrc, ssrc, wtgt, stgt)\n2: Align wsrc to wtgt with the Levenshtein algorithm [38], resulting in a transform falign(\u00b7)\n3: sali \u2190 falign(ssrc) \u25b7 sali is a speaker sequence of length M, and may contain blank\nspeakers \u2205 due to insertion errors in the alignment\n4: K \u2190 max{max(sali), max(stgt)} \u25b7 the maximal number of speakers in sali and stgt\n5: Initialize a cost matrix C \u2208 RK\u00d7K\n6: for 1 \u2264 i \u2264 K and 1 \u2264 j \u2264 K do\n7: Ci,j \u2190 P\n1\u2264m\u2264M \u03b4(sali\nm = i and stgt\nm = j)\n8: end for\n9: Solve the assignment problem with cost matrix C using the Hungarian algorithm [ 39],\nresulting in a transform fassign(\u00b7) \u25b7 handle speaker permutations\n10: for 1 \u2264 m \u2264 M do\n11: if sali\nm \u0338= \u2205 then\n12: stra\nm \u2190 fassign(sali\nm ) \u25b7 transfer the speakers from the source\n13: else\n14: stra\nm \u2190 stgt\nm \u25b7 preserve the target speaker if source speaker is unavailable\n15: end if\n16: end for\n17: end procedure\nBelow we show a simple example of the inputs and output of the TPS", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-22", "text": "13: else\n14: stra\nm \u2190 stgt\nm \u25b7 preserve the target speaker if source speaker is unavailable\n15: end if\n16: end for\n17: end procedure\nBelow we show a simple example of the inputs and output of the TPST algorithm:\nExample inputs and output of the TPST algorithm\nSource words: hello good morning hi how are you pretty good\nSource speakers: 1 1 1 2 2 2 2 1 1\nTarget words: hello morning hi hey are you be good\nTarget speakers: 1 2 2 2 1 1 2 1\nTransferred speakers: 1 1 2 2 2 2 1 1\n3.5 LLM FINETUNING\nAlthough the examples shown in Section 2 and Appendix B were using off-the-shelf Web APIs of\ncommercial LLMs, finetuning the LLM specifically on the speaker diarization task is still required if\nwe need to:\n1. Reduce errors of a specific speaker diarization system;\n2https://github.com/google/speaker-id/tree/master/DiarizationLM\n6\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\n2. Handle more complicated errors;\n3. Keep ASR transcripts unmodified as much as possible fro", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-23", "text": "/tree/master/DiarizationLM\n6\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\n2. Handle more complicated errors;\n3. Keep ASR transcripts unmodified as much as possible from the LLM outputs;\n4. Avoid undesired leading or tailing text from the generated completions, such as \u201cHere is the\ncorrected transcript\u201d or \u201cWe corrected the speakers for these words\u201d;\n5. Use smaller and cheaper LLMs.\nTo finetune the LLM, we build our training data as a collection of prompt-completion pairs. First,\nfor each utterance, we run the ASR model and the speaker diarization system on it, and apply the\norchestration module as shown in Fig. 2. This will produce the hypothesis word sequence whyp\nand hypothesis speaker sequence shyp. From the ground truth annotations of this utterance, we\nbuild the reference word sequence wref and the reference speaker sequence sref . Given these four\nsequences, we can build the prompts and completions in our training data with three different flavors", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-24", "text": "build the reference word sequence wref and the reference speaker sequence sref . Given these four\nsequences, we can build the prompts and completions in our training data with three different flavors,\nas introduced below.\n3.5.1 F LAVOR 1: HYP 2ORA\nThe first flavor is named hypothesis-to-oracle, or simply hyp2ora. In this flavor, we apply the\nTranscript-Preserving Speaker Transfer algorithm from Section 3.4 by treating reference sequences\nas source and hypothesis sequences as target:\nsora = TPST(wref , sref , whyp, shyp), (1)\nwhere the output sora is the oracle hypothesis speakers transferred from the reference sequences.\nWith sora, the prompts and completions in our training data are created as below:\n\u2022 Prompts: The text representation of whyp and shyp, with segmentation, and optionally prefix\nand suffix.\n\u2022 Completions: The text representation of whyp and sora, with segmentation, and optionally\nsuffix.\n3.5.2 F LAVOR 2: DEG 2REF\nThe second flavor is named degraded-to-reference, or simp", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-25", "text": "refix\nand suffix.\n\u2022 Completions: The text representation of whyp and sora, with segmentation, and optionally\nsuffix.\n3.5.2 F LAVOR 2: DEG 2REF\nThe second flavor is named degraded-to-reference, or simply deg2ref. In this flavor, we apply the\nTranscript-Preserving Speaker Transfer algorithm from Section 3.4 by treating hypothesis sequences\nas source and reference sequences as target:\nsdeg = TPST(whyp, shyp, wref , sref ), (2)\nwhere the output sdeg is the degraded reference speakers transferred from the hypothesis sequences.\nWith sdeg, the prompts and completions in our training data are created as below:\n\u2022 Prompts: The text representation of wref and sdeg, with segmentation, and optionally prefix\nand suffix.\n\u2022 Completions: The text representation of wref and sref , with segmentation, and optionally\nsuffix.\n3.5.3 F LAVOR 3: MIXED\nThe third flavor named mixed is simply the union of the prompts and completions from the previous\ntwo flavors. When building training batches, prompt-completion", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-26", "text": "ionally\nsuffix.\n3.5.3 F LAVOR 3: MIXED\nThe third flavor named mixed is simply the union of the prompts and completions from the previous\ntwo flavors. When building training batches, prompt-completion pairs from the two flavors are\ninterleaved.\nNote that for all three flavors, it is critical for the prompt and completion to use the same word\nsequence with different speaker sequences. This helps the LLM to focus on correcting the speaker\nlabels without modifying the ASR transcripts.\n7\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\n4 E XPERIMENTS\n4.1 D ATASETS\nTo finetune the LLM, we use the training subset of the Fisher corpus [40], which consists of 1,920\nhours of 11,527 conversations. The same train-test split of the Fisher dataset has been used in many\nprevious works [8, 17, 35, 41]\nFor evaluation, we use the testing subset of the Fisher corpus [ 40], as well as the testing subset\nof Callhome American English data [ 42]. The Fisher testing subset consist", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-27", "text": "ous works [8, 17, 35, 41]\nFor evaluation, we use the testing subset of the Fisher corpus [ 40], as well as the testing subset\nof Callhome American English data [ 42]. The Fisher testing subset consists of 28.7 hours of\n172 conversations3. The Callhome American English testing subset consists of 1.7 hours of 20\nconversations. Both datasets are in the telephone speech domain, and all conversations have 2\nspeakers.\n4.2 M ETRICS\nTo evaluate the diarization performance, we use two metrics: the Word Diarization Error Rate\n(WDER) [25] and the concatenated minimum-permutation word error rate (cpWER) [43]. To briefly\nrecap, WDER is defined as:\nWDER = SIS + CIS\nS + C , (3)\nwhere,\n1. SIS is the number of ASR Substitutions with Incorrect Speaker tokens.\n2. CIS is the number of Correct ASR words with Incorrect Speaker tokens.\n3. S is the number of ASR substitutions.\n4. C is the number of Correct ASR words.\nAnd cpWER is computed as follows:\n1. Concatenate all transcripts of each speaker for both ref", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-28", "text": "ncorrect Speaker tokens.\n3. S is the number of ASR substitutions.\n4. C is the number of Correct ASR words.\nAnd cpWER is computed as follows:\n1. Concatenate all transcripts of each speaker for both reference and hypothesis.\n2. Compute the WER between the reference and all possible speaker permutations of the\nhypothesis.\n3. Pick the lowest WER among all these permutations, which is assumed to be the best\npermutation.\nAll three metrics reported in this paper (WER, WDER, and cpWER) are micro metrics, i.e. both\nnumerators and denominators are aggregated on the entire dataset.\n4.3 M ODELS\nFor the ASR model in Fig. 2, we use a universal speech model (USM) [ 44] with 600 million\nparameters trained with the RNN-T loss [45]. For the speaker diarization model in Fig. 2, we use the\nturn-to-diarize system [7] with a multi-stage clustering setup [17] in our experiments, which is capable\nof diarizing hours of audio recordings in real time on a mobile device [24]. The number of speakers\nis unknown (fr", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-29", "text": "tem [7] with a multi-stage clustering setup [17] in our experiments, which is capable\nof diarizing hours of audio recordings in real time on a mobile device [24]. The number of speakers\nis unknown (from 1 to \u221e) to the speaker diarization system in all of our experiments. Specifically,\nfor the Fisher training set, the Fisher testing set, and the Callhome testing set, all utterances have a\nground truth number of speakers equal to two, but our turn-to-diarize system may predict any number\nof hypothesis speakers. The histogram of number of hypothesis speakers predicted by turn-to-diarize\non the Fisher training set is shown in Figure 3. Although our experiments are based on the USM +\nturn-to-diarize setup, we would like to point out that the proposed framework is very generic and\nshould work with other ASR or speaker diarization systems as well, such as variants of end-to-end\nspeaker diarization models [20, 21, 22, 23].\nFor the LLM in Fig. 2, we experiment with the PaLM 2-S model (\u201ctext-bis", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-30", "text": "th other ASR or speaker diarization systems as well, such as variants of end-to-end\nspeaker diarization models [20, 21, 22, 23].\nFor the LLM in Fig. 2, we experiment with the PaLM 2-S model (\u201ctext-bison\u201d model in Google\nCloud API) and the PaLM 2-L model (\u201ctext-unicorn\u201d model in Google Cloud API) [46]. We use\n3https://github.com/google/speaker-id/blob/master/publications/ScdLoss/\neval/fisher.txt\n8\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nFigure 3: The histogram for the number of hypothesis speakers predicted by the turn-to-diarize\nsystem on the Fisher training set. Note that the ground truth number of speakers is always two on the\nFisher dataset, but we do not constrain the number of speakers for the turn-to-diarize system.\nthe PaLM 2-S model as our foundation model, and finetune it on the dataset described in Section 4.1\nwith data processing steps described in Section 3.5. This model uses a sentence piece model (SPM)\nof 256k tokens as its tokenizer", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-31", "text": "dation model, and finetune it on the dataset described in Section 4.1\nwith data processing steps described in Section 3.5. This model uses a sentence piece model (SPM)\nof 256k tokens as its tokenizer [47]. During finetuning, we limit the LLM input size by 4,096 tokens,\nand segment our training and testing data accordingly. The PaLM 2-L model will only be used for\nzero-shot and one-shot experiments, as described in Section 4.4. We also experimented with open\nsource models such as Llama 2 [48] and Llama 3. Model details on how we finetune these models\nare provided in Appendix A.\nIn our prompt builder module, we use an empty prompt prefix, and a 5-character prompt suffix\n\u201c --> \u201d (note the two spaces around the arrow). For the completions in our training data, we use a\n6-character completion suffix \u201c [eod]\u201d (short for \u201cend of document\u201d; note the leading space). After\nprocessing the training data with the prompt builder module, we result in 13,430 prompt-completion\npairs for training in tot", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-32", "text": "uffix \u201c [eod]\u201d (short for \u201cend of document\u201d; note the leading space). After\nprocessing the training data with the prompt builder module, we result in 13,430 prompt-completion\npairs for training in total. The average length of a prompt is 2,371 SPM tokens, and the average\nlength of a completion is 2,329 tokens. The LLM is trained for 1,200 steps with a batch size of 16.\n4.4 Z ERO -SHOT AND ONE -SHOT BASELINES\nApart from finetuning the PaLM 2-S model on the speaker diarization task, we also experiment with\ndirectly using the PaLM 2-S and PaLM 2-L models on the speaker diarization task without finetuning.\nThis is more similar to the example we demonstrated in Section 2.\nFor the zero-shot setup, we use a prompt prefix that contains an instruction describing the task, as\nshown below.\nPrompt prefix for zero-shot\nIn the speaker diarization transcript below, some words are potentially misplaced.\nPlease correct those words and move them to the right speaker.\nDirectly show the corrected transcri", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-33", "text": "fix for zero-shot\nIn the speaker diarization transcript below, some words are potentially misplaced.\nPlease correct those words and move them to the right speaker.\nDirectly show the corrected transcript without explaining what changes were made or why\nyou made those changes.\\n\nFor the one-shot setup, the prompt prefix contains both the instruction describing the task, and also a\nsmall example, as shown below.\n9\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nPrompt prefix for one-shot\nIn the speaker diarization transcript below, some words are potentially misplaced. Please\ncorrect those words and move them to the right speaker. For example, given this input\ntranscript,\n<spk:1> How are you doing today? I <spk:2> am doing very well. How was everything at the\n<spk:1> party? Oh, the party? It was awesome. We had lots of fun. Good <spk:2> to hear!\nThe correct output transcript should be:\n<spk:1> How are you doing today? <spk:2> I am doing very well. How was eve", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-34", "text": "k:1> party? Oh, the party? It was awesome. We had lots of fun. Good <spk:2> to hear!\nThe correct output transcript should be:\n<spk:1> How are you doing today? <spk:2> I am doing very well. How was everything at the\nparty? <spk:1> Oh, the party? It was awesome. We had lots of fun. <spk:2> Good to hear!\nNow, please correct the transcript below.\\n\n4.5 E VALUATION RESULTS\nIn Table 1, we show the evaluation results of the USM + turn-to-diarize baseline together with the\noutputs post-processed by DiarizationLM. We report results for zero-shot, one-shot, and finetuning\non the diarization task with three different flavors.\nFor zero-shot and one-shot experiments with PaLM 2-S, we observe significantly worse WDER and\ncpWER performance compared with the baseline system, indicating the PaLM 2-S foundation model\ndoes not offer speaker diarization capabilities without finetuning. Zero-shot experiment with PaLM\n2-L model also shows bad performance, while one-shot experiment with PaLM 2-L model is muc", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-35", "text": "tion model\ndoes not offer speaker diarization capabilities without finetuning. Zero-shot experiment with PaLM\n2-L model also shows bad performance, while one-shot experiment with PaLM 2-L model is much\nbetter, but still worse than the baseline system. Our results indicate that the PaLM 2-L model with\none-shot is able to improve speaker diarization in relatively simple cases as shown in Section 2 and\nAppendix B. However, real world applications can be much more complicated with errors from both\nthe ASR system and the speaker diarization system. In such cases, even with one-shot, LLM can still\nintroduce even more errors to the results if not finetuned specifically on the speaker diarization task.\nOn both datasets, we observe big improvement of both WDER and cpWER with any of the three\nfinetuning flavors. Interesting, the biggest improvement is observed with the hyp2ora flavor, while\nthe smallest improvement is observed with the deg2ref flavor. Specifically for hyp2ora, we see a\nrel. 55.5", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-36", "text": "ning flavors. Interesting, the biggest improvement is observed with the hyp2ora flavor, while\nthe smallest improvement is observed with the deg2ref flavor. Specifically for hyp2ora, we see a\nrel. 55.5% improvement of WDER after post-processing with DiarizationLM on the Fisher testing\nset. Even if we did not use any Callhome data during the LLM finetuning, we see a rel. 44.9%\nimprovement of WDER on the Callhome testing set. The WER of the USM on the two testing sets\nare relatively high due to domain mismatch and suboptimal annotation quality of the ground truth.\nHowever, this also demonstrated that the DiarizationLM solution provides consistent quality gains\neven with out-of-domain ASR and speaker diarization models.\nTo further demonstrate this, in Table 2, we show the results of a similar setup, but we replace the\nUSM-based ASR model directly by the ground truth ASR transcripts from the testing sets. For these\nexperiments, we will have WER=0%, and the hyp2ora and deg2ref flavors will b", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-37", "text": "setup, but we replace the\nUSM-based ASR model directly by the ground truth ASR transcripts from the testing sets. For these\nexperiments, we will have WER=0%, and the hyp2ora and deg2ref flavors will be equivalent. From\nthe table, we can still see big improvements of WDER after post-processing the diarization results by\nthe same DiarizationLM model (i.e. deg2ref flavor in Table 1).\n4.6 C ASE STUDIES\nBased on the results from Table 1, we also present example cases from the Fisher and Callhome\ntesting sets where we see big improvements of WDER in Table 3 and Table 4, respectively. From\nthese examples, we are seeing multiple patterns of corrections:\n\u2022 DiarizationLM make corrections where different parts of sentence are moved to the same\nspeaker, e.g. \u201cit\u2019s more of\u201d and \u201cit\u2019ll be warm\u201d in fe_03_07146 from Table 3. This is\nconsistent with our initial observations as demonstrated in Section 2.\n\u2022 DiarizationLM can merge short speaker turns due to disfluency, such as \u201cyeah yeah\u201d and \u201ci\ni hear i", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-38", "text": "_07146 from Table 3. This is\nconsistent with our initial observations as demonstrated in Section 2.\n\u2022 DiarizationLM can merge short speaker turns due to disfluency, such as \u201cyeah yeah\u201d and \u201ci\ni hear i hear \u201d in fe_03_11159 from Table. 3. Diarization errors from disfluency usually\nattribute to low quality speaker embeddings extracted from very short speaker turn segments.\n10\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nTable 1: Evaluation results of the USM + turn-to-diarize baseline system and the results post-\nprocessed by DiarizationLM. For DiarizationLM, we experiment with PaLM 2 foundation models\nwith and without finetuning on the diarization task. We also experiment with Llama 2 and Llama 3\nmodels finetuned on the diarization task (model details in Appendix A). WERs are the same for all\nsystems due to TPST. All numbers are percentages.\nSystem Fisher testing set Callhome testing set\nWER WDER cpWER WER WDER cpWER\nUSM + turn-to-diarize baseline 15.48", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-39", "text": "endix A). WERs are the same for all\nsystems due to TPST. All numbers are percentages.\nSystem Fisher testing set Callhome testing set\nWER WDER cpWER WER WDER cpWER\nUSM + turn-to-diarize baseline 15.48 5.32 21.19 15.36 7.72 24.39\n+ PaLM 2-S zero-shot - 11.96 30.19 - 12.26 30.60\n+ PaLM 2-S one-shot - 16.58 38.03 - 14.50 34.32\n+ PaLM 2-L zero-shot - 11.36 31.78 - 13.29 34.30\n+ PaLM 2-L one-shot - 5.94 22.21 - 7.95 24.67\n+ PaLM 2-S finetuned (hyp2ora) - 2.37 16.93 - 4.25 20.22\n+ PaLM 2-S finetuned (deg2ref) - 3.94 18.55 - 5.33 21.47\n+ PaLM 2-S finetuned (mixed) - 2.41 16.94 - 4.76 20.84\n+ Llama 2 13B finetuned (mixed) v1 - 3.65 18.92 - 8.02 25.11\n+ Llama 3 8B finetuned (mixed) v1 - 4.40 19.76 - 12.27 30.80\n+ Llama 3 8B finetuned (mixed) v2 - 3.28 18.37 - 6.66 23.57\nTable 2: Evaluation results of the turn-to-diarize baseline system with reference ASR transcript\n(assuming WER=0%) and the results post-processed by DiarizationLM. For DiarizationLM, we\nexperiment with PaLM 2 foundation models wi", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-40", "text": "f the turn-to-diarize baseline system with reference ASR transcript\n(assuming WER=0%) and the results post-processed by DiarizationLM. For DiarizationLM, we\nexperiment with PaLM 2 foundation models with and without finetuning on the diarization task. All\nnumbers are percentages.\nSystem Fisher testing set Callhome testing set\nWDER cpWER WDER cpWER\nReference + turn-to-diarize baseline 2.81 5.19 3.74 6.82\n+ PaLM 2-S zero-shot 7.50 12.70 7.29 12.79\n+ PaLM 2-S one-shot 10.92 19.16 12.79 21.65\n+ PaLM 2-L zero-shot 8.69 16.85 11.67 22.87\n+ PaLM 2-L one-shot 3.23 5.99 3.76 6.95\n+ PaLM 2-S finetuned 1.18 2.21 1.49 2.66\n\u2022 DiarizationLM can also detect speaker turns due to interruptions, such as \u201coh all right\u201d in\nfe_03_11210 from Table 3, and \u201coh my\u201d in en_6408 from Table 4.\nWe also look into why zero-shot and one-shot experiments in Table 1 produced worse results than\nthe baseline system. We found that without finetuning on the speaker diarization tasks, zero-shot\nand one-shot outputs from the L", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-41", "text": "o-shot and one-shot experiments in Table 1 produced worse results than\nthe baseline system. We found that without finetuning on the speaker diarization tasks, zero-shot\nand one-shot outputs from the LLM often delete big chunks of hypothesis text from the prompt.\nFinetuning the LLM is critical to avoid such undesired deletions. A few zero-shot examples with the\nPaLM 2-S model from the Fisher testing set were shown in Table 5.\n5 D ISCUSSION AND FUTURE WORK\nThe experiments in Section 4 have shown very promising results where LLMs can significantly\nreduce speaker diarization errors. However, we also admit the limitations of these experiments. First\nof all, the training and testing data from the experiments are all based on the telephone speech domain,\nall with exactly 2 speakers. An important future work would be to include more diverse datasets to\nfinetune the LLM, and evaluate its performance across different domains with unknown number of\nspeakers.\nIn Appendix B, we have demonstrated ot", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-42", "text": "future work would be to include more diverse datasets to\nfinetune the LLM, and evaluate its performance across different domains with unknown number of\nspeakers.\nIn Appendix B, we have demonstrated other diarization capabilities of LLMs. However, due to lack\nof relevant datasets, we haven\u2019t been able to thoroughly evaluate these capabilities. One interesting\nfuture work would be to collect datasets of these tasks and evaluate how LLM performs.\n11\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nTable 3: Example cases from the Fisher testing set where we see big absolute WDER reduction (\u2206\nWDER) with DiarizationLM (deg2ref flavor).\nUtterance Before DiarizationLM After DiarizationLM\nfe_03_07146\n(\u2206 WDER\n=8.80%)\n...\n<spk:3> it\u2019s it\u2019s\n<spk:1> more of summer always like you\nknow we never experience a bit cold over\nhere\n<spk:4> usually it\u2019ll\n<spk:1> be warm or like very hot in summer\nyeah and\n<spk:3> extremely hot yeah with high humid-\nity my humidity is pretty\n<sp", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-43", "text": "ke you\nknow we never experience a bit cold over\nhere\n<spk:4> usually it\u2019ll\n<spk:1> be warm or like very hot in summer\nyeah and\n<spk:3> extremely hot yeah with high humid-\nity my humidity is pretty\n<spk:1> much high because i stay close to\nthe sea coast over here\n<spk:3> yeah\n<spk:1> so\n<spk:3> that makes you live houston is it like\nhouston where you live yeah i i i live\n<spk:1> in houston\n...\n...\n<spk:1> it\u2019s it\u2019s more of summer always like\nyou know we never experience a bit cold over\nhere usually it\u2019ll be warm or like very hot in\nsummer\n<spk:2> yeah and extremely hot yeah with\nhigh humidity my\n<spk:1> humidity is pretty much high be-\ncause i stay close to the sea coast over here\n<spk:2> yeah so that makes you live houston\nis it like houston where you live\n<spk:1> yeah i i i live in houston\n...\nfe_03_06816\n(\u2206 WDER\n=6.61%)\n...\n<spk:3> uhuh\n<spk:2> did you see the the woman golfer\nthat was on this the one\n<spk:1> monica yeah yeah\n<spk:2> what\u2019s her name monica stone yeah\nmhm she she\n<spk", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-44", "text": "n\n...\nfe_03_06816\n(\u2206 WDER\n=6.61%)\n...\n<spk:3> uhuh\n<spk:2> did you see the the woman golfer\nthat was on this the one\n<spk:1> monica yeah yeah\n<spk:2> what\u2019s her name monica stone yeah\nmhm she she\n<spk:1> blew out she fell out of that tourna-\nment but i didn\u2019t think she\u2019d do it she she\u2019s\ngirls can\u2019t compete against guys\n...\n...\n<spk:2> uhuh did you see the the woman\ngolfer that was on this the one\n<spk:1> monica yeah yeah\n<spk:2> what\u2019s her name monica stone\n<spk:1> yeah\n<spk:2> mhm\n<spk:1> she she blew out she fell out of that\ntournament but i didn\u2019t think she\u2019d do it she\nshe\u2019s girls can\u2019t compete against guys\n...\nfe_03_11210\n(\u2206 WDER\n=6.35%)\n...\n<spk:1> the vikings mine\u2019s the eagles i\u2019m\nfrom new jersey oh all right i have my jersey\non now i watch the game tonight yeah well i\ni may i may just watch\n<spk:2> part of it tonight too then but uh it\u2019s a\ncase as i say if if i had to pay for it i probably\nwouldn\u2019t watch it\n<spk:1> i wouldn\u2019t either uhhuh\n<spk:2> unless\n<spk:1> it was an eagles", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-45", "text": "st watch\n<spk:2> part of it tonight too then but uh it\u2019s a\ncase as i say if if i had to pay for it i probably\nwouldn\u2019t watch it\n<spk:1> i wouldn\u2019t either uhhuh\n<spk:2> unless\n<spk:1> it was an eagles game\n...\n...\n<spk:1> the vikings mine\u2019s the eagles i\u2019m\nfrom new jersey\n<spk:2> oh all right\n<spk:1> i have my jersey on now i watch the\ngame tonight yeah\n<spk:2> well i i may i may just watch part of\nit tonight too then but uh it\u2019s a case as i say\nif if i had to pay for it i probably wouldn\u2019t\nwatch it\n<spk:1> i wouldn\u2019t either\n<spk:2> uhhuh\n<spk:1> unless it was an eagles game\n...\nfe_03_11159\n(\u2206 WDER\n=4.05%)\n...\n<spk:2> yeah\n<spk:1> anniversary that\u2019s horrible\n<spk:2> yeah\n<spk:1> yeah it\u2019s not good\n<spk:2> i\n<spk:1> i hear i hear you there that\u2019s not a\ngood thing you\n<spk:2> know i mean of course you know\nthat\u2019s a day that will go down instantly no-\nbody will ever remember it\n...\n...\n<spk:1> yeah anniversary that\u2019s horrible yeah\nyeah it\u2019s not good i i hear i hear you there\nthat\u2019s not a go", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-46", "text": "urse you know\nthat\u2019s a day that will go down instantly no-\nbody will ever remember it\n...\n...\n<spk:1> yeah anniversary that\u2019s horrible yeah\nyeah it\u2019s not good i i hear i hear you there\nthat\u2019s not a good thing\n<spk:2> you know i mean of course you know\nthat\u2019s a day that will go down instantly no-\nbody will ever remember it\n...\n12\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nTable 4: Example cases from the Callhome testing set where we see big absolute WDER reduction\n(\u2206 WDER) with DiarizationLM (deg2ref flavor).\nUtterance Before DiarizationLM After DiarizationLM\nen_6447\n(\u2206 WDER\n=12.49%)\n...\n<spk:1> i\u2019m\n<spk:2> going to see if i can talk to the guy\nthat\u2019s selling the trailer if i can chew him\ndown a bit uhhuh\n<spk:1> and\n<spk:2> you know what you just said bene-\ndicta is are you living with benedicta\n<spk:1> yes yes yes\n<spk:2> you know what i bet she answered\nthe phone\n...\n...\n<spk:2> i\u2019m going to see if i can talk to the\nguy that\u2019s selling the trailer if", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-47", "text": "e-\ndicta is are you living with benedicta\n<spk:1> yes yes yes\n<spk:2> you know what i bet she answered\nthe phone\n...\n...\n<spk:2> i\u2019m going to see if i can talk to the\nguy that\u2019s selling the trailer if i can chew him\ndown a bit\n<spk:1> uhhuh\n<spk:2> and you know what you just said\nbenedicta is are you living with benedicta\n<spk:1> yes yes yes\n<spk:2> you know what i bet she answered\nthe phone\n...\nen_6408\n(\u2206 WDER\n=10.87%)\n...\n<spk:1> uhu\n<spk:2> so\n<spk:1> he had big surgery again and he\u2019s in\na wheelchair oh my\n<spk:2> and\n<spk:1> he doesn\u2019t want to go to school in a\nwheelchair uhuh but\n<spk:2> he might he wants to have tutoring\nat home but they\u2019re still where they lived on\n45th street\n<spk:1> yeah they\u2019re there\n...\n...\n<spk:2> uhu\n<spk:1> so he had big surgery again and he\u2019s\nin a wheelchair\n<spk:2> oh my\n<spk:1> and he doesn\u2019t want to go to school\nin a wheelchair\n<spk:2> uhuh\n<spk:1> but he might he wants to have tutor-\ning at home\n<spk:2> but they\u2019re still where they lived on\n45th stre", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-48", "text": "<spk:2> oh my\n<spk:1> and he doesn\u2019t want to go to school\nin a wheelchair\n<spk:2> uhuh\n<spk:1> but he might he wants to have tutor-\ning at home\n<spk:2> but they\u2019re still where they lived on\n45th street\n<spk:1> yeah they\u2019re there\n...\nen_6298\n(\u2206 WDER\n=9.95%)\n...\n<spk:1> um hey we\u2019re we\u2019re confused about\nyou guys address\n<spk:2> is\n<spk:1> it 1324 or 13\n<spk:2> it\u2019s 1 324\n<spk:1> excuse me 1324 yes and it\u2019s me view\nis me two words or one word yes it\u2019s two\nwords and there\u2019s an ln besides\n...\n...\n<spk:1> um hey we\u2019re we\u2019re confused about\nyou guys address is it 1324 or 13\n<spk:2> it\u2019s 1 324\n<spk:1> excuse me 1324\n<spk:2> yes\n<spk:1> and it\u2019s me view is me two words or\none word\n<spk:2> yes it\u2019s two words and there\u2019s an ln\nbesides\n...\nen_4792\n(\u2206 WDER\n=9.42%)\n...\n<spk:2> yeah well he was at columbia\n<spk:1> he was there like five years and they\nturned him down for tenure then he went\nsomewhere else he he was down in college\npark maryland yeah and he i think he was\nonly non tenure track down th", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-49", "text": "ia\n<spk:1> he was there like five years and they\nturned him down for tenure then he went\nsomewhere else he he was down in college\npark maryland yeah and he i think he was\nonly non tenure track down there then sup-\nposedly supposed to be back in japan now\nyeah but you know he\u2019s he\u2019s probably be-\ncome an english teacher at some unit yeah\ni know a guy believe it or not i know a guy\nfrom manhattan who was up in sapotto his\nmajor he did an mba believe it or not he\u2019s\nhe\u2019s an english teacher now huh\n...\n...\n<spk:2> yeah well he was at columbia\n<spk:1> he was there like five years and they\nturned him down for tenure then he went\nsomewhere else he he was down in college\npark maryland\n<spk:2> yeah\n<spk:1> and he i think he was only non tenure\ntrack down there then supposedly supposed\nto be back in japan now\n<spk:2> yeah\n<spk:1> but you know he\u2019s he\u2019s probably be-\ncome an english teacher at some unit\n<spk:2> yeah\n<spk:1> i know a guy believe it or not i know\na guy from manhattan who was up in sap", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-50", "text": "apan now\n<spk:2> yeah\n<spk:1> but you know he\u2019s he\u2019s probably be-\ncome an english teacher at some unit\n<spk:2> yeah\n<spk:1> i know a guy believe it or not i know\na guy from manhattan who was up in sapotto\nhis major he did an mba believe it or not he\u2019s\nhe\u2019s an english teacher now\n<spk:2> huh\n...\n13\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nTable 5: Example cases from the Fisher testing set where zero-shot PaLM 2-S deletes lots of text\nfrom the prompt.\nUtterance Before DiarizationLM After DiarizationLM\nfe_03_11252 ...\n<spk:1> oh okay i believe it\u2019s a lot wrong\nwith the public schools i don\u2019t believe that\nthey\u2019re um that they\u2019re giving um these kids\na sense of um well they\u2019re not teaching them\nwhat they need to know once they get out of\num school you know mhm um what what\u2019s\nhappening is that\u2019s probably why you got a\nlot of um a lot of people that\u2019s unemployed\ni think you know they you get a lot from\nschool and they taking a lot of um i guess the\neconomi", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-51", "text": "hm um what what\u2019s\nhappening is that\u2019s probably why you got a\nlot of um a lot of people that\u2019s unemployed\ni think you know they you get a lot from\nschool and they taking a lot of um i guess the\neconomics out of school you know\n<spk:2> right\n...\n...\n<spk:1> oh okay i believe it\u2019s a lot wrong\nwith the public schools i don\u2019t believe that\nthey\u2019re um that they\u2019re giving um these kids\na sense of um well they\u2019re not teaching them\nwhat they need to know once they get out of\num school you know\n<spk:2> right\n...\nfe_03_11224 ...\n<spk:1> so um i think what do you think is\nan important thing in a relation i think the\ntopic was um what you um what are the most\nimportant things in a life partner yeah uh h\nwell what do you think me\n<spk:2> i would have to say trust and honesty\nlike cuz without that you really don\u2019t have\nnothing to build on you know right yeah\n...\n...\n<spk:1> so um i think what do you think is\nan important thing in a relation\n<spk:2> i would have to say trust and honesty\nlike cuz withou", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-52", "text": "lly don\u2019t have\nnothing to build on you know right yeah\n...\n...\n<spk:1> so um i think what do you think is\nan important thing in a relation\n<spk:2> i would have to say trust and honesty\nlike cuz without that you really don\u2019t have\nnothing to build on you know right\n...\nAnother research direction would be to compare different LLMs, in different size variants on the\nspeaker diarization task. Specifically, the performance will likely be even better if we finetune larger\nmodels such as PaLM 2-M or PaLM 2-L. It would also be interesting to reproduce the experiments\nwith other speaker diarization systems such as EEND [20] or WEEND [34].\nLastly, as PaLM 2 models are multilingual [46], the DiarizationLM framework can naturally apply\nto speaker diarization tasks in other languages. It would be helpful to evaluate how DiarizationLM\nperforms on speaker diarization datasets in other languages than English.\n6 R ELATED WORK\n6.1 S PEAKER DIARIZATION POST -PROCESSING\nIn the context of conventional speak", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-53", "text": "l to evaluate how DiarizationLM\nperforms on speaker diarization datasets in other languages than English.\n6 R ELATED WORK\n6.1 S PEAKER DIARIZATION POST -PROCESSING\nIn the context of conventional speaker diarization, \u201cpost-processing\u201d usually refers to a stage where\nthe clustering results are refined with signals from other sources or systems. An early post-processing\napproach was known as \u201cresegmentation\u201d, where the Gaussian mixture models (GMMs) are estimated\nfor each speaker with the Baum-Welch algorithm, and a Viterbi algorithm is used to re-annotate\nthe speakers with the GMMs [49]. Later in [ 50], the authors proposed to use a neural network for\nresegmentation, with an additional class for non-speech. In [51], the authors proposed DiaCorrect,\na method inspired by error correction techniques in ASR. DiaCorrect uses parallel convolutional\nencoders for the speakers from the initial diarization results and a transformer based decoder to\nproduce corrected diarization results. One major", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-54", "text": "iques in ASR. DiaCorrect uses parallel convolutional\nencoders for the speakers from the initial diarization results and a transformer based decoder to\nproduce corrected diarization results. One major difference in our proposed framework is that\nwe leverage semantic information to refine the diarization results on a word level, while these\nresegmentation approaches are only based on acoustic information and perform at cluster level.\nAnother type of post-processing is to combine the outputs of multiple speaker diarization systems,\ne.g. via majority voting [52], speaker matching [53], or both [54]. More recently in [16], the authors\nproposed to perform speaker diarization on different temporal scales, and combine their outputs\nvia 1-D convolutional neural networks. In [ 55], the authors proposed to use end-to-end speaker\ndiarization as a post-processing step for initial speaker diarization results of a clustering-based system.\n14\nDiarizationLM: Speaker Diarization Post-Processing with Lar", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-55", "text": "sed to use end-to-end speaker\ndiarization as a post-processing step for initial speaker diarization results of a clustering-based system.\n14\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nOur proposed framework is generic such that it can apply to either the results of a single speaker\ndiarization system, or to the combined results of multiple speaker diarization systems.\n6.2 S PEAKER DIARIZATION WITH SEMANTIC INFORMATION\nApart from the joint ASR and speaker diarization models discussed in Section 1, researchers have also\nstudied various approaches of integrating semantic information into conventional speaker diarization\nsystems. Some of the benefits of DiarizationLM may also be achieved with non-LLM methods.\nThe most common approach to leverage semantic information is to use ASR word alignments to\nrefine the voice activity detection and initial segmentation [ 56]. A variant of this approach is to\nbuild a speaker turn detection model and segment by speake", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-56", "text": "rmation is to use ASR word alignments to\nrefine the voice activity detection and initial segmentation [ 56]. A variant of this approach is to\nbuild a speaker turn detection model and segment by speaker turns [57]. In [58], a Gated Recurrent\nUnits (GRUs) [59] based speaker turn probability estimator is trained on top of word embeddings\nand speaker embeddings, and the estimated probabilities are combined with the adjacency matrix for\nspectral clustering. Similarly in [7], an end-to-end trained transformer transducer (T-T) [60] based\nspeaker turn detection model is used to constrain the spectral clustering via Exhaustive and Efficient\nConstraint Propagation (E2CP).\n6.3 S PEAKER DIARIZATION WITH LLM\nIn [ 35], the authors proposed Speaker Error Corrector (SEC), which aims to solve the same problem\nas we stated in Section 1. In [ 35], word embeddings from the ASR transcript are extracted with a\npre-trained Roberta-base LM [61]. Then a separately trained transformer encoder takes the word\nemb", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-57", "text": "lem\nas we stated in Section 1. In [ 35], word embeddings from the ASR transcript are extracted with a\npre-trained Roberta-base LM [61]. Then a separately trained transformer encoder takes the word\nembeddings and the hypothesis speaker labels as input, and produces the corrected speaker labels.\nThe transformer encoder is trained on both simulated diarization errors and real data. The biggest\ndifference from our proposed framework to [35] is that we directly feed the compact pure textual\nrepresentation of the ASR and diarization results as part of the prompt to the LLM, and directly\nfinetune the LLM to produce the corrected results in the same compact textual representation. Our\nDiarizationLM is a \u201ctext-in, text-out\u201d system, without relying on internal embedding representations\nfrom the LLM.\nMore recently in [ 62], the authors proposed to use LLM to predict the speaker probability for the\nnext word, and incorporate this probability into the beam search decoding of speaker diarization.\nOu", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-58", "text": "More recently in [ 62], the authors proposed to use LLM to predict the speaker probability for the\nnext word, and incorporate this probability into the beam search decoding of speaker diarization.\nOur proposed framework differs from this work by using a single prompt (or several prompts due to\nLLM input size limit) to post-process the entire results of the speaker diarization system, instead of\nword-by-word prompting. Additionally, our proposed framework can be more generally applied to\nany speaker diarization system, instead of requiring word-level speaker probabilities for beam search\ndecoding.\n7 C ONCLUSION\nIn this paper, we demonstrate that large language models (LLM) can be used to post-process speaker\ndiarization results, achieving various goals such as improving the readability of the diarization\ntranscript, and reducing the diarization errors. Specifically, we proposed DiarizationLM, a framework\nwhere we use a finetuned LLM to refine the results from off-the-shelf ASR and spea", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-59", "text": "the diarization\ntranscript, and reducing the diarization errors. Specifically, we proposed DiarizationLM, a framework\nwhere we use a finetuned LLM to refine the results from off-the-shelf ASR and speaker diarization\nsystems. We introduced three different flavors to build the prompt-completion pairs data for finetuning\nthe LLM. Our experiments on Fisher and Callhome datasets show that a finetuned PaLM 2-S model\ncan drastically reduce the word diarization error rates of typical diarization systems like turn-to-\ndiarize.\nREFERENCES\n[1] Tae Jin Park, Naoyuki Kanda, Dimitrios Dimitriadis, Kyu J Han, Shinji Watanabe, and Shrikanth\nNarayanan, \u201cA review of speaker diarization: Recent advances with deep learning,\u201d Computer\nSpeech & Language, vol. 72, pp. 101317, 2022.\n[2] Chao Zhang and Quan Wang, \u201cSpeaker diarization: A journey from unsupervised to supervised\napproaches,\u201d Odyssey: The Speaker and Language Recognition Workshop, 2022, Tutorial\nsession.\n15\nDiarizationLM: Speaker Diarization Post-", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-60", "text": "\u201cSpeaker diarization: A journey from unsupervised to supervised\napproaches,\u201d Odyssey: The Speaker and Language Recognition Workshop, 2022, Tutorial\nsession.\n15\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\n[3] Rub\u00e9n Zazo Candil, Tara N Sainath, Gabor Simko, and Carolina Parada, \u201cFeature learning with\nraw-waveform CLDNNs for voice activity detection,\u201d in Proc. Interspeech, 2016.\n[4] Ivan Medennikov, Maxim Korenevsky, Tatiana Prisyach, Yuri Khokhlov, Mariya Korenevskaya,\nIvan Sorokin, Tatiana Timofeeva, Anton Mitrofanov, Andrei Andrusenko, Ivan Podluzhny, et al.,\n\u201cTarget-speaker voice activity detection: a novel approach for multi-speaker diarization in a\ndinner party scenario,\u201d in Proc. Interspeech, 2020.\n[5] Shaojin Ding, Quan Wang, Shuo-yiin Chang, Li Wan, and Ignacio Lopez Moreno, \u201cPersonal\nV AD: Speaker-conditioned voice activity detection,\u201d inOdyssey: The Speaker and Language\nRecognition Workshop, 2020.\n[6] Shaojin Ding, Rajeev Rikhye, Qiao Liang, Ya", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-61", "text": "and Ignacio Lopez Moreno, \u201cPersonal\nV AD: Speaker-conditioned voice activity detection,\u201d inOdyssey: The Speaker and Language\nRecognition Workshop, 2020.\n[6] Shaojin Ding, Rajeev Rikhye, Qiao Liang, Yanzhang He, Quan Wang, Arun Narayanan, Tom\nO\u2019Malley, and Ian McGraw, \u201cPersonal vad 2.0: Optimizing personal voice activity detection for\non-device speech recognition,\u201d arXiv preprint arXiv:2204.03793, 2022.\n[7] Wei Xia, Han Lu, Quan Wang, Anshuman Tripathi, Yiling Huang, Ignacio Lopez Moreno, and\nHasim Sak, \u201cTurn-to-Diarize: Online speaker diarization constrained by transformer transducer\nspeaker turn detection,\u201d inInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2022, pp. 8077\u20138081.\n[8] Guanlong Zhao, Quan Wang, Han Lu, Yiling Huang, and Ignacio Lopez Moreno, \u201cAugmenting\ntransformer-transducer based speaker change detection with token-level training loss,\u201d in\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023.\n[9] Li", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-62", "text": "\u201cAugmenting\ntransformer-transducer based speaker change detection with token-level training loss,\u201d in\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023.\n[9] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno, \u201cGeneralized end-to-end loss for\nspeaker verification,\u201d in International Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2018, pp. 4879\u20134883.\n[10] Chao Li, Xiaokong Ma, Bing Jiang, Xiangang Li, Xuewei Zhang, Xiao Liu, Ying Cao, Ajay\nKannan, and Zhenyao Zhu, \u201cDeep speaker: an end-to-end neural speaker embedding system,\u201d\narXiv preprint arXiv:1705.02304, 2017.\n[11] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khudanpur,\n\u201cX-Vectors: Robust dnn embeddings for speaker recognition,\u201d in International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5329\u20135333.\n[12] Quan Wang, Carlton Downey, Li Wan, Philip Andrew Mansfield, and Ignacio Lopez Moreno,\n\u201cSpeaker diar", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-63", "text": "onal Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5329\u20135333.\n[12] Quan Wang, Carlton Downey, Li Wan, Philip Andrew Mansfield, and Ignacio Lopez Moreno,\n\u201cSpeaker diarization with LSTM,\u201d in International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE, 2018, pp. 5239\u20135243.\n[13] Daniel Garcia-Romero, David Snyder, Gregory Sell, Daniel Povey, and Alan McCree, \u201cSpeaker\ndiarization using deep neural network embeddings,\u201d in International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2017, pp. 4930\u20134934.\n[14] Dimitrios Dimitriadis and Petr Fousek, \u201cDeveloping on-line speaker diarization system,\u201d in\nProc. Interspeech, 2017, pp. 2739\u20132743.\n[15] Tae Jin Park, Kyu J Han, Manoj Kumar, and Shrikanth Narayanan, \u201cAuto-tuning spectral clus-\ntering for speaker diarization using normalized maximum eigengap,\u201d IEEE Signal Processing\nLetters, vol. 27, pp. 381\u2013385, 2019.\n[16] Tae Jin Park, Manoj Kumar, and Shrikanth Narayanan, \u201cM", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-64", "text": "pectral clus-\ntering for speaker diarization using normalized maximum eigengap,\u201d IEEE Signal Processing\nLetters, vol. 27, pp. 381\u2013385, 2019.\n[16] Tae Jin Park, Manoj Kumar, and Shrikanth Narayanan, \u201cMulti-scale speaker diarization with\nneural affinity score fusion,\u201d in International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE, 2021, pp. 7173\u20137177.\n[17] Quan Wang, Yiling Huang, Han Lu, Guanlong Zhao, and Ignacio Lopez Moreno, \u201cHighly\nefficient real-time streaming and fully on-device speaker diarization with multi-stage clustering,\u201d\narXiv preprint arXiv:2210.13690, 2022.\n[18] Aonan Zhang, Quan Wang, Zhenyao Zhu, John Paisley, and Chong Wang, \u201cFully supervised\nspeaker diarization,\u201d in International Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2019, pp. 6301\u20136305.\n[19] Qiujia Li, Florian L Kreyssig, Chao Zhang, and Philip C Woodland, \u201cDiscriminative neural\nclustering for speaker diarisation,\u201d in Spoken Language Technology Workshop (SLT). IE", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-65", "text": "2019, pp. 6301\u20136305.\n[19] Qiujia Li, Florian L Kreyssig, Chao Zhang, and Philip C Woodland, \u201cDiscriminative neural\nclustering for speaker diarisation,\u201d in Spoken Language Technology Workshop (SLT). IEEE,\n2021.\n[20] Yusuke Fujita, Naoyuki Kanda, Shota Horiguchi, Kenji Nagamatsu, and Shinji Watanabe,\n\u201cEnd-to-end neural speaker diarization with permutation-free objectives,\u201d in Proc. Interspeech,\n2019, pp. 4300\u20134304.\n16\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\n[21] Yushi Ueda, Soumi Maiti, Shinji Watanabe, Chunlei Zhang, Meng Yu, Shi-Xiong Zhang, and\nYong Xu, \u201cEEND-SS: Joint end-to-end neural speaker diarization and speech separation for\nflexible number of speakers,\u201d arXiv preprint arXiv:2203.17068, 2022.\n[22] Quan Wang, Yash Sheth, Ignacio Lopez Moreno, and Li Wan, \u201cSpeaker diarization using an\nend-to-end model,\u201d US Patent US011545157B2, 2019.\n[23] Shota Horiguchi, Yusuke Fujita, Shinji Watanabe, Yawen Xue, and Kenji Nagamatsu, \u201cEnd-\nto-end speaker dia", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-66", "text": "d Li Wan, \u201cSpeaker diarization using an\nend-to-end model,\u201d US Patent US011545157B2, 2019.\n[23] Shota Horiguchi, Yusuke Fujita, Shinji Watanabe, Yawen Xue, and Kenji Nagamatsu, \u201cEnd-\nto-end speaker diarization for an unknown number of speakers with encoder-decoder based\nattractors,\u201d arXiv preprint arXiv:2005.09921, 2020.\n[24] Quan Wang and Fan Zhang, \u201cWho said what? Recorder\u2019s on-device solution for labeling\nspeakers,\u201d Google AI Blog.\n[25] Laurent El Shafey, Hagen Soltau, and Izhak Shafran, \u201cJoint speech recognition and speaker\ndiarization via sequence transduction,\u201d in Proc. Interspeech, 2019, pp. 396\u2013400.\n[26] Naoyuki Kanda, Yashesh Gaur, Xiaofei Wang, Zhong Meng, Zhuo Chen, Tianyan Zhou, and\nTakuya Yoshioka, \u201cJoint speaker counting, speech recognition, and speaker identification for\noverlapped speech of any number of speakers,\u201d arXiv preprint arXiv:2006.10930, 2020.\n[27] Naoyuki Kanda, Zhong Meng, Liang Lu, Yashesh Gaur, Xiaofei Wang, Zhuo Chen, and\nTakuya Yoshioka, \u201cMinimum bayes ri", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-67", "text": "lapped speech of any number of speakers,\u201d arXiv preprint arXiv:2006.10930, 2020.\n[27] Naoyuki Kanda, Zhong Meng, Liang Lu, Yashesh Gaur, Xiaofei Wang, Zhuo Chen, and\nTakuya Yoshioka, \u201cMinimum bayes risk training for end-to-end speaker-attributed ASR,\u201d in\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021,\npp. 6503\u20136507.\n[28] Naoyuki Kanda, Guoli Ye, Yashesh Gaur, Xiaofei Wang, Zhong Meng, Zhuo Chen, and\nTakuya Yoshioka, \u201cEnd-to-end speaker-attributed ASR with transformer,\u201d arXiv preprint\narXiv:2104.02128, 2021.\n[29] Naoyuki Kanda, Jian Wu, Yu Wu, Xiong Xiao, Zhong Meng, Xiaofei Wang, Yashesh Gaur, Zhuo\nChen, Jinyu Li, and Takuya Yoshioka, \u201cStreaming speaker-attributed ASR with token-level\nspeaker embeddings,\u201d arXiv preprint arXiv:2203.16685, 2022.\n[30] Katerina Zmolikova, Marc Delcroix, Keisuke Kinoshita, Takuya Higuchi, Atsunori Ogawa, and\nTomohiro Nakatani, \u201cSpeaker-aware neural network based beamformer for speaker extraction\nin speech mixtures,\u201d", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-68", "text": "Katerina Zmolikova, Marc Delcroix, Keisuke Kinoshita, Takuya Higuchi, Atsunori Ogawa, and\nTomohiro Nakatani, \u201cSpeaker-aware neural network based beamformer for speaker extraction\nin speech mixtures,\u201d in Proc. Interspeech, 2017, pp. 2655\u20132659.\n[31] Marc Delcroix, Katerina Zmolikova, Keisuke Kinoshita, Atsunori Ogawa, and Tomohiro\nNakatani, \u201cSingle channel target speaker extraction and recognition with speaker beam,\u201d in\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018,\npp. 5554\u20135558.\n[32] Marc Delcroix, Shinji Watanabe, Tsubasa Ochiai, Keisuke Kinoshita, Shigeki Karita, Atsunori\nOgawa, and Tomohiro Nakatani, \u201cEnd-to-end SpeakerBeam for single channel target speech\nrecognition,\u201d in Proc. Interspeech, 2019, pp. 451\u2013455.\n[33] Naoyuki Kanda, Shota Horiguchi, Ryoichi Takashima, Yusuke Fujita, Kenji Nagamatsu, and\nShinji Watanabe, \u201cAuxiliary interference speaker loss for target-speaker speech recognition,\u201d\narXiv preprint arXiv:1906.10876, 2019.\n[34] Yil", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-69", "text": "hi, Ryoichi Takashima, Yusuke Fujita, Kenji Nagamatsu, and\nShinji Watanabe, \u201cAuxiliary interference speaker loss for target-speaker speech recognition,\u201d\narXiv preprint arXiv:1906.10876, 2019.\n[34] Yiling Huang, Weiran Wang, Guanlong Zhao, Hank Liao, Wei Xia, and Quan Wang, \u201cOn\nthe success and limitations of auxiliary network based word-level end-to-end neural speaker\ndiarization,\u201d in Proc. Interspeech, 2024, pp. 32\u201336.\n[35] Rohit Paturi, Sundararajan Srinivasan, and Xiang Li, \u201cLexical speaker error correction: Leverag-\ning language models for speaker diarization error correction,\u201d arXiv preprint arXiv:2306.09313,\n2023.\n[36] James Manyika and Sissie Hsiao, \u201cAn overview of Bard: an early experiment with generative AI,\u201d\nhttps://ai.google/static/documents/google-about-bard.pdf, 2023.\n[37] OpenAI, \u201cIntroducing ChatGPT,\u201d https://openai.com/blog/chatgpt, 2022.\n[38] Vladimir I Levenshtein, \u201cBinary codes capable of correcting deletions, insertions, and reversals,\u201d\nSoviet physics doklady, vol. 1", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-70", "text": "AI, \u201cIntroducing ChatGPT,\u201d https://openai.com/blog/chatgpt, 2022.\n[38] Vladimir I Levenshtein, \u201cBinary codes capable of correcting deletions, insertions, and reversals,\u201d\nSoviet physics doklady, vol. 10, no. 8, pp. 707\u2013710, 1966.\n[39] Harold W Kuhn, \u201cThe Hungarian method for the assignment problem,\u201d Naval research logistics\nquarterly, vol. 2, no. 1-2, pp. 83\u201397, 1955.\n[40] Christopher Cieri, David Miller, and Kevin Walker, \u201cThe Fisher corpus: A resource for the next\ngenerations of speech-to-text,\u201d in LREC, 2004, vol. 4, pp. 69\u201371.\n17\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\n[41] Guanlong Zhao, Yongqiang Wang, Jason Pelecanos, Yu Zhang, Hank Liao, Yiling Huang,\nHan Lu, and Quan Wang, \u201cUSM-SCD: Multilingual speaker change detection based on large\npretrained foundation models,\u201d arXiv preprint arXiv:2309.08023, 2023.\n[42] A Canavan, D Graff, and G Zipperlen, \u201cCALLHOME American English speech LDC97S42,\u201d\nLDC Catalog. Philadelphia: Linguistic Data Consortiu", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-71", "text": "d foundation models,\u201d arXiv preprint arXiv:2309.08023, 2023.\n[42] A Canavan, D Graff, and G Zipperlen, \u201cCALLHOME American English speech LDC97S42,\u201d\nLDC Catalog. Philadelphia: Linguistic Data Consortium, 1997.\n[43] Shinji Watanabe, Michael Mandel, Jon Barker, Emmanuel Vincent, Ashish Arora, Xuankai\nChang, Sanjeev Khudanpur, Vimal Manohar, Daniel Povey, Desh Raj, et al., \u201cCHiME-6\nchallenge: Tackling multispeaker speech recognition for unsegmented recordings,\u201d arXiv\npreprint arXiv:2004.09249, 2020.\n[44] Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen,\nBo Li, Vera Axelrod, Gary Wang, et al., \u201cGoogle USM: Scaling automatic speech recognition\nbeyond 100 languages,\u201d arXiv preprint arXiv:2303.01037, 2023.\n[45] Alex Graves, \u201cSequence transduction with recurrent neural networks,\u201d arXiv preprint\narXiv:1211.3711, 2012.\n[46] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhi", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-72", "text": "neural networks,\u201d arXiv preprint\narXiv:1211.3711, 2012.\n[46] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al., \u201cPaLM 2 technical report,\u201d\narXiv preprint arXiv:2305.10403, 2023.\n[47] Taku Kudo and John Richardson, \u201cSentencePiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing,\u201d arXiv preprint arXiv:1808.06226, 2018.\n[48] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al., \u201cLlama 2: Open\nfoundation and fine-tuned chat models,\u201d arXiv preprint arXiv:2307.09288, 2023.\n[49] Gregory Sell and Daniel Garcia-Romero, \u201cDiarization resegmentation in the factor analysis\nsubspace,\u201d in International Conference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2015, pp. 4794\u20134798.\n[50] Ruiqing Yin, Herv\u00e9 Bredin, and Claude Barras,", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-73", "text": "ntation in the factor analysis\nsubspace,\u201d in International Conference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2015, pp. 4794\u20134798.\n[50] Ruiqing Yin, Herv\u00e9 Bredin, and Claude Barras, \u201cNeural speech turn segmentation and affinity\npropagation for speaker diarization,\u201d in Proc. Interspeech, 2018, pp. 1393\u20131397.\n[51] Jiangyu Han, Federico Landini, Johan Rohdin, Mireia Diez, Lukas Burget, Yuhang Cao, Heng\nLu, and Jan Cernocky, \u201cDiaCorrect: Error correction back-end for speaker diarization,\u201d arXiv\npreprint arXiv:2309.08377, 2023.\n[52] MAH Huijbregts, David A van Leeuwen, and FM Jong, \u201cThe majority wins: a method for\ncombining speaker diarization systems,\u201d in Proc. Interspeech, 2009.\n[53] Simon Bozonnet, Nicholas Evans, Xavier Anguera, Oriol Vinyals, Gerald Friedland, and\nCorinne Fredouille, \u201cSystem output combination for improved speaker diarization,\u201d in Proc.\nInterspeech, 2010.\n[54] Andreas Stolcke and Takuya Yoshioka, \u201cDOVER: A method for combining diarization outputs,\u201d\ni", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-74", "text": "e Fredouille, \u201cSystem output combination for improved speaker diarization,\u201d in Proc.\nInterspeech, 2010.\n[54] Andreas Stolcke and Takuya Yoshioka, \u201cDOVER: A method for combining diarization outputs,\u201d\nin Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp.\n757\u2013763.\n[55] Shota Horiguchi, Paola Garcia, Yusuke Fujita, Shinji Watanabe, and Kenji Nagamatsu, \u201cEnd-to-\nend speaker diarization as post-processing,\u201d in International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). IEEE, 2021, pp. 7188\u20137192.\n[56] Jan Silovsky, Jindrich Zdansky, Jan Nouza, Petr Cerva, and Jan Prazak, \u201cIncorporation of\nthe ASR output in speaker segmentation and clustering within the task of speaker diarization\nof broadcast streams,\u201d in International Workshop on Multimedia Signal Processing (MMSP).\nIEEE, 2012, pp. 118\u2013123.\n[57] Tae Jin Park and Panayiotis Georgiou, \u201cMultimodal speaker segmentation and diarization using\nlexical and acoustic cues via sequence to sequence neural n", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-75", "text": "rocessing (MMSP).\nIEEE, 2012, pp. 118\u2013123.\n[57] Tae Jin Park and Panayiotis Georgiou, \u201cMultimodal speaker segmentation and diarization using\nlexical and acoustic cues via sequence to sequence neural networks,\u201d inProc. Interspeech, 2018,\npp. 1373\u20131377.\n[58] Tae Jin Park, Kyu J Han, Jing Huang, Xiaodong He, Bowen Zhou, Panayiotis Georgiou,\nand Shrikanth Narayanan, \u201cSpeaker diarization with lexical information,\u201d arXiv preprint\narXiv:2004.06756, 2020.\n18\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\n[59] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio, \u201cEmpirical evaluation\nof gated recurrent neural networks on sequence modeling,\u201d arXiv preprint arXiv:1412.3555,\n2014.\n[60] Qian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi, Erik McDermott, Stephen Koo, and\nShankar Kumar, \u201cTransformer transducer: A streamable speech recognition model with\ntransformer encoders and RNN-T loss,\u201d in International Conference on Acoustics, Speech and\nSignal Processi", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-76", "text": "n Koo, and\nShankar Kumar, \u201cTransformer transducer: A streamable speech recognition model with\ntransformer encoders and RNN-T loss,\u201d in International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2020, pp. 7829\u20137833.\n[61] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov, \u201cRoberta: A robustly optimized bert\npretraining approach,\u201d arXiv preprint arXiv:1907.11692, 2019.\n[62] Tae Jin Park, Kunal Dhawan, Nithin Koluguri, and Jagadeesh Balam, \u201cEnhancing speaker\ndiarization with large language models: A contextual beam search approach,\u201d arXiv preprint\narXiv:2309.05248, 2023.\n[63] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen, \u201cLoRA: Low-rank adaptation of large language models,\u201d arXiv\npreprint arXiv:2106.09685, 2021.\n19\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nAppendices\nA O PEN SOURCE M", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-77", "text": "LoRA: Low-rank adaptation of large language models,\u201d arXiv\npreprint arXiv:2106.09685, 2021.\n19\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nAppendices\nA O PEN SOURCE MODELS\nApart from PaLM 2 [ 46] models, we have also experimented with open source models such as\nLlama 2 [48] and Llama 3. These finetuned models are publicly available on Hugging Face. The\nfinetuning scripts are available at: https://github.com/google/speaker-id/tree/\nmaster/DiarizationLM/unsloth.\nA.1 google/DiarizationLM-13b-Fisher-v1\nModel URL: https://huggingface.co/google/\nDiarizationLM-13b-Fisher-v1\nFoundation model: https://huggingface.co/unsloth/\nllama-2-13b-bnb-4bit\nThis model is finetuned on the training subset of the Fisher corpus, using a LoRA adapter [ 63] of\nrank 256. The total number of training parameters is 1,001,390,080. With a batch size of 16, this\nmodel has been trained for 12000 steps, which is \u223c4 epochs of the training data.\nWe use the \u201cmixed\u201d flavor during our traini", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-78", "text": "mber of training parameters is 1,001,390,080. With a batch size of 16, this\nmodel has been trained for 12000 steps, which is \u223c4 epochs of the training data.\nWe use the \u201cmixed\u201d flavor during our training, meaning we combine data from \u201chyp2ora\u201d and\n\u201cdeg2ref\u201d flavors (see Section 3.5 for the definition of these flavors). After the prompt builder, we\nhave a total of 48,142 prompt-completion pairs in our training set.\nThe finetuning took more than 3 days on a Google Cloud VM instance that has one NVIDIA A100\nGPU with 80GB memory.\nThe maximal length of the prompt to this model is 6000 characters, including the \u201c --> \u201d suffix.\nThe maximal sequence length is 4096 tokens.\nThe evaluation results of this model is also provided in Table 1. We can see that similar to the\nfinetuned PaLM 2-S models, this model also significantly reduces the WDER and cpWER on the\nFisher testing set. However, on the Callhome testing set, we are seeing degradation in WDER\nand cpWER, indicating overfitting on the Fisher", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-79", "text": "model also significantly reduces the WDER and cpWER on the\nFisher testing set. However, on the Callhome testing set, we are seeing degradation in WDER\nand cpWER, indicating overfitting on the Fisher dataset. We have found this is because the loss is\ncomputed on both the prompt and the completion tokens during training.\nA.2 google/DiarizationLM-8b-Fisher-v1\nModel URL: https://huggingface.co/google/\nDiarizationLM-8b-Fisher-v1\nFoundation model: https://huggingface.co/unsloth/\nllama-3-8b-bnb-4bit\nThis model is finetuned on the training subset of the Fisher corpus, using a LoRA adapter [ 63] of\nrank 256. The total number of training parameters is 671,088,640. With a batch size of 16, this model\nhas been trained for 25400 steps, which is \u223c8 epochs of the training data.\nOther configurations of this model is the same as the model in the previous section. Note that we still\nuse a maximal sequence length of 4096 tokens even if Llama 3 supports longer sequence.\nThe finetuning took more than 4 da", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-80", "text": "his model is the same as the model in the previous section. Note that we still\nuse a maximal sequence length of 4096 tokens even if Llama 3 supports longer sequence.\nThe finetuning took more than 4 days on a Google Cloud VM instance that has one NVIDIA A100\nGPU with 80GB memory.\nThe evaluation results of this model is also provided in Table 1. We can see that its performance is\nsignificantly worse than the Llama 2 13B model from Appendix A.1, but still better than the USM +\nturn-to-diarize baseline on Fisher testing set. Similarly, the performance on Callhome is worse than\nbaseline, because the loss is computed on both the prompt and the completion tokens during training.\n20\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nA.3 google/DiarizationLM-8b-Fisher-v2\nModel URL: https://huggingface.co/google/\nDiarizationLM-8b-Fisher-v1\nFoundation model: https://huggingface.co/unsloth/\nllama-3-8b-bnb-4bit\nThis model is finetuned on the training subset of the Fisher", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-81", "text": "Model URL: https://huggingface.co/google/\nDiarizationLM-8b-Fisher-v1\nFoundation model: https://huggingface.co/unsloth/\nllama-3-8b-bnb-4bit\nThis model is finetuned on the training subset of the Fisher corpus, using a LoRA adapter [ 63] of\nrank 256. The total number of training parameters is 671,088,640. With a batch size of 16, this model\nhas been trained for 28800 steps, which is \u223c9 epochs of the training data.\nNote that when training this model, the loss is only computed on the completion tokens .\nThis is the biggest different between this model and the previous model in Appendix A.2 (\ngoogle/DiarizationLM-8b-Fisher-v1).\nOther configurations of this model is the same as the model in the previous section. Note that we still\nuse a maximal sequence length of 4096 tokens even if Llama 3 supports longer sequence.\nThe finetuning took more than 4 days on a Google Cloud VM instance that has one NVIDIA A100\nGPU with 80GB memory.\nThe evaluation results of this model is also provided in Table 1.", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-82", "text": "rts longer sequence.\nThe finetuning took more than 4 days on a Google Cloud VM instance that has one NVIDIA A100\nGPU with 80GB memory.\nThe evaluation results of this model is also provided in Table 1. We can see that its performance is\nsignificantly better than the model from Appendix A.2. Specifically, we no longer see degradation on\nCallhome testin set.\nB O THER DIARIZATION CAPABILITIES OF LLM S\nIn our studies, we have found multiple diarization capabilities of LLMs. Due to the lack of relevant\ntraining and testing datasets, this paper mainly focuses on evaluating how LLMs can reduce diariza-\ntion errors. In this section, we provide several examples to demonstrate LLM\u2019s other diarization\ncapabilities. All the provided examples are obtained by directly sending the prompts to Google\nBard [36]4.\nB.1 A UTOFILLING SPEAKER NAMES\nIn real world applications, real speaker names or aliases are much more readable than anonymous\nspeaker labels such as \u201cspeaker 1\u201d or \u201cspeaker 2\u201d. In many cases, s", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-83", "text": "36]4.\nB.1 A UTOFILLING SPEAKER NAMES\nIn real world applications, real speaker names or aliases are much more readable than anonymous\nspeaker labels such as \u201cspeaker 1\u201d or \u201cspeaker 2\u201d. In many cases, speaker names can be obtained\nfrom the context of the conversation and from additional metadata of the conversation. In meeting\nsummarization, the names can be obtained from the list of meeting attendants. And for video\ncaptioning, the names may be extracted from the video title or description. An example is provided\nbelow.\nPrompt:\n[video title] Tom meets Patrick in the parking lot\n[video description] A short conversation\nReplace the <spk:?> tokens from the below video caption with actual person names.\n<spk:1> Good morning Patrick, how are you?\n<spk:2> Good, good. How are you Tom?\n<spk:1> Pretty good. Going to work?\n<spk:2> Yes. Busy day. How are your kids? Do they go to school?\n<spk:1> Oh they are too young for that. I sent them to daycare earlier today.\n<spk:2> Oh yeah I forgot about that", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-84", "text": "y good. Going to work?\n<spk:2> Yes. Busy day. How are your kids? Do they go to school?\n<spk:1> Oh they are too young for that. I sent them to daycare earlier today.\n<spk:2> Oh yeah I forgot about that.\nCompletion:\n4We used an internal version of Bard that is based on a larger model and supports more tokens than the public\nversion.\n21\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nBased on the video title \"Tom meets Patrick in the parking lot\", here\u2019s the transcript with speaker\nnames replaced:\nTom: Good morning Patrick, how are you?\nPatrick: Good, good. How are you Tom?\nTom: Pretty good. Going to work?\nPatrick: Yes. Busy day. How are your kids? Do they go to school?\nTom: Oh they are too young for that. I sent them to daycare earlier today.\nPatrick: Oh yeah, I forgot about that.\nB.2 A UTOFILLING SPEAKER ROLES\nIn some applications, knowing the roles of each speaker is useful enough. For example, it would be\nhelpful to distinguish the doctor from the patient", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-85", "text": "forgot about that.\nB.2 A UTOFILLING SPEAKER ROLES\nIn some applications, knowing the roles of each speaker is useful enough. For example, it would be\nhelpful to distinguish the doctor from the patient in medical transcripts [25], and to distinguish the\ncustomer from the agent in call center conversations. This task can be handled with LLM usually\nwithout additional metadata about the conversation. We show an example below.\nPrompt:\nReplace the <spk:?> tokens from the below conversation with roles like teacher, student, doctor,\npatient, etc.\n<spk:1> Hi, how can I help you today?\n<spk:2> Hi, I recently often feel quite dizzy at work.\n<spk:1> Do you have any of these symptoms? Like coughing, fever, or running nose?\n<spk:2> No, I don\u2019t think so.\n<spk:1> What do you usually eat for breakfast, lunch and dinner?\n<spk:2> I\u2019m on a diet, so not eating very much. Mostly just one slice of toast in the morning,\nand eating salad for lunch and dinner.\nCompletion:\nDoctor: Hi, how can I help you today?", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-86", "text": "nch and dinner?\n<spk:2> I\u2019m on a diet, so not eating very much. Mostly just one slice of toast in the morning,\nand eating salad for lunch and dinner.\nCompletion:\nDoctor: Hi, how can I help you today?\nPatient: Hi, I recently often feel quite dizzy at work.\nDoctor: Do you have any of these symptoms? Like coughing, fever, or running nose?\nPatient: No, I don\u2019t think so.\nDoctor: What do you usually eat for breakfast, lunch, and dinner?\nPatient: I\u2019m on a diet, so not eating very much. Mostly just one slice of toast in the morning,\nand eating salad for lunch and dinner.\nB.3 R EPLACING THE ORCHESTRATION MODULE\nIn Fig. 1, we have explained how an orchestration module can combine the ASR transcripts with\nspeaker diarization outputs via the timing information from the two systems. Interestingly, we have\nfound that this process can also be fully replaced by LLM. This can be achieved by explicitly including\nthe timing information in the textual representation of both ASR transcripts and speaker dia", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-87", "text": "ave\nfound that this process can also be fully replaced by LLM. This can be achieved by explicitly including\nthe timing information in the textual representation of both ASR transcripts and speaker diarization\noutputs. Additionally, more prompt engineering will be needed, such as explicitly explaining the\nformat of the textual representation, and providing a one-shot example in the prompt. We show an\nexample below.\nPrompt:\nHere we define the problem of speaker-transcript alignment. The transcript is represented by\nmultiple entries of text, where each text has a starting time and an ending time. The speaker is\nalso represented in this format. The alignment problem will assign a speaker to each word in the\ntext, based on which speaker overlaps the most with that word in time.\n22\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nBelow is an example.\nTranscript represented in format \"[start - end] text\":\n[0 - 2.3] Good morning Patrick\n[2.5 - 5.2] how are you?\n[5.", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-88", "text": ": Speaker Diarization Post-Processing with Large Language Models\nBelow is an example.\nTranscript represented in format \"[start - end] text\":\n[0 - 2.3] Good morning Patrick\n[2.5 - 5.2] how are you?\n[5.6 - 6.1] Good, good.\n[6.2 - 8.3] How are you Tom?\n[9.2 - 9.9] Pretty good.\n[10.0 - 11.1] Going to work?\n[12.5 - 13.6] Yes. Busy day.\nSpeaker represented in format \"[start - end] speaker\":\n[0 - 5.1] <spk:1>\n[5.3 - 8.7] <spk:2>\n[9.2 - 10.9] <spk:1>\n[12.1 - 13.5] <spk:2>\nThe alignment result should be:\n[0 - 2.3] <spk:1> Good morning Patrick\n[2.5 - 5.2] <spk:1> how are you?\n[5.6 - 6.1] <spk:2> Good, good.\n[6.2 - 8.3] <spk:2> How are you Tom?\n[9.2 - 9.9] <spk:1> Pretty good.\n[10.0 - 11.1] <spk:1> Going to work?\n[12.5 - 13.6] <spk:2> Yes. Busy day.\nBased on the example above, we solve the alignment problem below.\nTranscript represented in format \"[start - end] text\":\n[0 - 5.6] Hi, how can I help you today?\n[6.2 - 11.1] Hi, I recently often feel quite dizzy at work.\n[11.6 - 15.5] Do you have any", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-89", "text": "problem below.\nTranscript represented in format \"[start - end] text\":\n[0 - 5.6] Hi, how can I help you today?\n[6.2 - 11.1] Hi, I recently often feel quite dizzy at work.\n[11.6 - 15.5] Do you have any of these symptoms?\n[16.6 - 18.5] Like coughing, fever, or running nose?\n[20.0 - 21.1] No, I don\u2019t think so.\n[22.2 - 29.9] What do you usually eat for breakfast, lunch and dinner?\n[31.2 - 34.8] I\u2019m on a diet, so not eating very much.\n[35.2 - 39.8] Mostly just one slice of toast in the morning, and eating salad for lunch and dinner.\nSpeaker represented in format \"[start - end] speaker\":\n[0.3 - 5.3] <spk:1>\n[6.0 - 12.0] <spk:2>\n[12.9 - 20.1] <spk:1>\n[20.2 - 21.0] <spk:2>\n[21.8 - 31.1] <spk:1>\n[32.4 - 40.7] <spk:2>\nThe alignment result should be:\nCompletion:\nHere\u2019s the aligned transcript:\n[0 - 5.6] <spk:1> Hi, how can I help you today?\n[6.2 - 11.1] <spk:2> Hi, I recently often feel quite dizzy at work.\n[11.6 - 15.5] <spk:1> Do you have any of these symptoms?\n[16.6 - 18.5] <spk:1> Like coughing", "title": "DIARIZATIONLM.pdf"}
{"chunk_id": "DIARIZATIONLM.pdf-90", "text": "] <spk:1> Hi, how can I help you today?\n[6.2 - 11.1] <spk:2> Hi, I recently often feel quite dizzy at work.\n[11.6 - 15.5] <spk:1> Do you have any of these symptoms?\n[16.6 - 18.5] <spk:1> Like coughing, fever, or running nose?\n[20.0 - 21.1] <spk:2> No, I don\u2019t think so.\n23\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\n[22.2 - 29.9] <spk:1> What do you usually eat for breakfast, lunch and dinner?\n[31.2 - 34.8] <spk:2> I\u2019m on a diet, so not eating very much.\n[35.2 - 39.8] <spk:2> Mostly just one slice of toast in the morning, and eating salad for lunch\nand dinner.\nTo align the transcript and speaker segments, we assign each word in the transcript to\nthe speaker whose segment overlaps the most with that word in time.\n24", "title": "DIARIZATIONLM.pdf"}
